{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84Q8JfvaeZZ6"
      },
      "source": [
        "## Linear Classifier in TensorFlow \n",
        "Using Low Level API in Eager Execution mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sb7Epo0VOB58"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpCNRv1OB5-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mjtb-EMcm5K0",
        "colab": {}
      },
      "source": [
        "#Enable Eager Execution if using tensflow version < 2.0\n",
        "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDmJqqOB6K"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhllFLyKOB6N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d66334d-319d-4d31-f38c-d0139e577370"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KiObW4V4SIOz",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4yQKMiJOB6R",
        "colab": {}
      },
      "source": [
        "data_I = pd.read_csv('/content/drive/My Drive/AIML_Training/Iris.csv')\n",
        "data_P = pd.read_csv('/content/drive/My Drive/AIML_Training/prices.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgkX6SEqOB6W"
      },
      "source": [
        "### Check all columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7K8pWsNQOB6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "34010b3f-3059-4c93-e9c1-14e79283750d"
      },
      "source": [
        "data_P.head()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt3oI4Of4q0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e6edd1e7-54c4-4c1f-c737-8fa7d6b339d2"
      },
      "source": [
        "data_I.head()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
              "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dU6X7MpOB6c"
      },
      "source": [
        "### Drop columns `date` and  `symbol`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lh_6spSKOB6e",
        "colab": {}
      },
      "source": [
        "data_P1 = data_P.drop(['date', 'symbol'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlwbUgTwOB6i",
        "outputId": "edf2a09e-af4b-41dc-ef2f-2a39d2a77ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_P1.head()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DBv3WWYOB6q"
      },
      "source": [
        "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
        "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_hG9rGBOB6s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "336fd7e0-d16a-4254-8121-005602f17d44"
      },
      "source": [
        "data_P1.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(851264, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep6f_0y26bIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a65bd032-2eff-4199-e497-1601b0dc9a38"
      },
      "source": [
        "data_P = data_P1.head(1000)\n",
        "data_P.shape"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U39kPJsv7mSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "81f31929-3282-4c6e-d1fa-31c444172723"
      },
      "source": [
        "data_P.dtypes"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "open      float64\n",
              "close     float64\n",
              "low       float64\n",
              "high      float64\n",
              "volume    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKD3PtNycUr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f7e94cc7-3463-4a63-9b44-c64e284a66ed"
      },
      "source": [
        "data_P['volume'] = data_P['volume']/1000000"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1or84-z9mzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1f9637d2-38ab-48bf-c660-9c852bc51d8a"
      },
      "source": [
        "data_P.head()\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2.1636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2.3864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2.4895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2.0063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1.4086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high  volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2.1636\n",
              "1  125.239998  119.980003  119.940002  125.540001  2.3864\n",
              "2  116.379997  114.949997  114.930000  119.739998  2.4895\n",
              "3  115.480003  116.620003  113.500000  117.440002  2.0063\n",
              "4  117.010002  114.970001  114.089996  117.330002  1.4086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3UaApqYOB6x"
      },
      "source": [
        "### Divide the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LE4U8lTdQJq",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU71XB-59LUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = data_P[:,0:4]\n",
        "#Y = data_P[:,4]\n",
        "\n",
        "X = data_P.drop(\"volume\", axis=1)\n",
        "Y = data_P[\"volume\"]\n",
        "# split into 70% for train and 30% for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYK-aUuLbrz2"
      },
      "source": [
        "#### Convert Training and Test Data to numpy float32 arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ao-S0tQGcncz",
        "colab": {}
      },
      "source": [
        "X_train = np.array(X_train, dtype=\"float32\")\n",
        "X_test = np.array(X_test, dtype=\"float32\")\n",
        "y_train = np.array(y_train, dtype=\"float32\")\n",
        "y_test = np.array(y_test, dtype=\"float32\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZDbtaroDMV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA7CrFxUCSUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "im1ZegbDdKgv"
      },
      "source": [
        "### Normalize the data\n",
        "You can use Normalizer from sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EkKAy7fOB6y",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "#S=StandardScaler()\n",
        "#S.fit(data_P1)\n",
        "#X_train=S.fit_transform(X_train)\n",
        "#X_test=S.fit_transform(X_test)\n",
        "\n",
        "transformer = Normalizer().fit(X_train)\n",
        "X_train = transformer.transform(X_train)\n",
        "X_test =transformer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6vE4eYCOB62"
      },
      "source": [
        "## Building the Model in tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "297_qja4OB7A"
      },
      "source": [
        "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L205qPeQOB7B",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\n",
        "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HgtWA-UIOB7F"
      },
      "source": [
        "2.Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRd-eu72GiRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input features\n",
        "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\n",
        "\n",
        "#Normalize the data\n",
        "x_n = tf.nn.l2_normalize(x,1)\n",
        "\n",
        "#Actual Prices\n",
        "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JveGlx25OB7H",
        "colab": {}
      },
      "source": [
        "#We will use normalized data\n",
        "#y = tf.add(tf.matmul(x,W),b,name='output')\n",
        "y = tf.add(tf.matmul(x_n,W),b,name='output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TL1hIwf_OB7M"
      },
      "source": [
        "3.Loss (Cost) Function [Mean square error]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYyAUtA6Xyqo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VSWPiGXOB7P",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzG85FUlOB7U"
      },
      "source": [
        "4.Function to train the Model\n",
        "\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj802w-3OB7X",
        "colab": {}
      },
      "source": [
        "train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSypb_u8OB7e"
      },
      "source": [
        "## Train the model for 100 epochs \n",
        "1. Observe the training loss at every iteration\n",
        "2. Observe Train loss at every 5th iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVvgj7eQOB7f",
        "colab": {}
      },
      "source": [
        "#Lets start graph Execution\n",
        "sess = tf.Session()\n",
        "\n",
        "# variables need to be initialized before we can use them\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#how many times data need to be shown to model\n",
        "training_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3QaD-LiP_SN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b452ac6f-b916-4c98-97a4-61315ca6e9d0"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "            \n",
        "    #Calculate train_op and loss\n",
        "    _, train_loss = sess.run([train_op,loss],feed_dict={x:X_train, y_:y_train})\n",
        "    \n",
        "    if epoch % 1 == 0:\n",
        "        print ('Training loss at step: ', epoch, ' is ', train_loss)\n",
        "sess.close()        "
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at step:  0  is  305.429\n",
            "Training loss at step:  1  is  298.32846\n",
            "Training loss at step:  2  is  292.8297\n",
            "Training loss at step:  3  is  288.5715\n",
            "Training loss at step:  4  is  285.27386\n",
            "Training loss at step:  5  is  282.72018\n",
            "Training loss at step:  6  is  280.74258\n",
            "Training loss at step:  7  is  279.21118\n",
            "Training loss at step:  8  is  278.0252\n",
            "Training loss at step:  9  is  277.1068\n",
            "Training loss at step:  10  is  276.39557\n",
            "Training loss at step:  11  is  275.8448\n",
            "Training loss at step:  12  is  275.41827\n",
            "Training loss at step:  13  is  275.08798\n",
            "Training loss at step:  14  is  274.83215\n",
            "Training loss at step:  15  is  274.6341\n",
            "Training loss at step:  16  is  274.4807\n",
            "Training loss at step:  17  is  274.36194\n",
            "Training loss at step:  18  is  274.2699\n",
            "Training loss at step:  19  is  274.1987\n",
            "Training loss at step:  20  is  274.14352\n",
            "Training loss at step:  21  is  274.10077\n",
            "Training loss at step:  22  is  274.06772\n",
            "Training loss at step:  23  is  274.04205\n",
            "Training loss at step:  24  is  274.02225\n",
            "Training loss at step:  25  is  274.0069\n",
            "Training loss at step:  26  is  273.99496\n",
            "Training loss at step:  27  is  273.98575\n",
            "Training loss at step:  28  is  273.97864\n",
            "Training loss at step:  29  is  273.97308\n",
            "Training loss at step:  30  is  273.9688\n",
            "Training loss at step:  31  is  273.9655\n",
            "Training loss at step:  32  is  273.96298\n",
            "Training loss at step:  33  is  273.96094\n",
            "Training loss at step:  34  is  273.95944\n",
            "Training loss at step:  35  is  273.95825\n",
            "Training loss at step:  36  is  273.95728\n",
            "Training loss at step:  37  is  273.95657\n",
            "Training loss at step:  38  is  273.95605\n",
            "Training loss at step:  39  is  273.9556\n",
            "Training loss at step:  40  is  273.95526\n",
            "Training loss at step:  41  is  273.95502\n",
            "Training loss at step:  42  is  273.95483\n",
            "Training loss at step:  43  is  273.95468\n",
            "Training loss at step:  44  is  273.95456\n",
            "Training loss at step:  45  is  273.95444\n",
            "Training loss at step:  46  is  273.95438\n",
            "Training loss at step:  47  is  273.9543\n",
            "Training loss at step:  48  is  273.95428\n",
            "Training loss at step:  49  is  273.95428\n",
            "Training loss at step:  50  is  273.95422\n",
            "Training loss at step:  51  is  273.95422\n",
            "Training loss at step:  52  is  273.9542\n",
            "Training loss at step:  53  is  273.95416\n",
            "Training loss at step:  54  is  273.95416\n",
            "Training loss at step:  55  is  273.95416\n",
            "Training loss at step:  56  is  273.95416\n",
            "Training loss at step:  57  is  273.95416\n",
            "Training loss at step:  58  is  273.95416\n",
            "Training loss at step:  59  is  273.95416\n",
            "Training loss at step:  60  is  273.95416\n",
            "Training loss at step:  61  is  273.95413\n",
            "Training loss at step:  62  is  273.95416\n",
            "Training loss at step:  63  is  273.95416\n",
            "Training loss at step:  64  is  273.95413\n",
            "Training loss at step:  65  is  273.95413\n",
            "Training loss at step:  66  is  273.95416\n",
            "Training loss at step:  67  is  273.95413\n",
            "Training loss at step:  68  is  273.95413\n",
            "Training loss at step:  69  is  273.95413\n",
            "Training loss at step:  70  is  273.95416\n",
            "Training loss at step:  71  is  273.95416\n",
            "Training loss at step:  72  is  273.95416\n",
            "Training loss at step:  73  is  273.95413\n",
            "Training loss at step:  74  is  273.95413\n",
            "Training loss at step:  75  is  273.95413\n",
            "Training loss at step:  76  is  273.95413\n",
            "Training loss at step:  77  is  273.95413\n",
            "Training loss at step:  78  is  273.95413\n",
            "Training loss at step:  79  is  273.95416\n",
            "Training loss at step:  80  is  273.95416\n",
            "Training loss at step:  81  is  273.95416\n",
            "Training loss at step:  82  is  273.95416\n",
            "Training loss at step:  83  is  273.95413\n",
            "Training loss at step:  84  is  273.95413\n",
            "Training loss at step:  85  is  273.95413\n",
            "Training loss at step:  86  is  273.95413\n",
            "Training loss at step:  87  is  273.95413\n",
            "Training loss at step:  88  is  273.95413\n",
            "Training loss at step:  89  is  273.95413\n",
            "Training loss at step:  90  is  273.95413\n",
            "Training loss at step:  91  is  273.95413\n",
            "Training loss at step:  92  is  273.95413\n",
            "Training loss at step:  93  is  273.95413\n",
            "Training loss at step:  94  is  273.95413\n",
            "Training loss at step:  95  is  273.95413\n",
            "Training loss at step:  96  is  273.95413\n",
            "Training loss at step:  97  is  273.95413\n",
            "Training loss at step:  98  is  273.95413\n",
            "Training loss at step:  99  is  273.95413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68AV8FrhKdnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2bac74e8-9ecb-4b01-e05e-72c46f09e9b9"
      },
      "source": [
        "#Lets start graph Execution\n",
        "sess = tf.Session()\n",
        "\n",
        "# variables need to be initialized before we can use them\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#how many times data need to be shown to model\n",
        "training_epochs = 100\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "            \n",
        "    #Calculate train_op and loss\n",
        "    _, train_loss = sess.run([train_op,loss],feed_dict={x:X_train, y_:y_train})\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        print ('Training loss at step: ', epoch, ' is ', train_loss)\n",
        "        print ('W values :' , sess.run(W))\n",
        "        print ('b values :' , sess.run(b))\n",
        "        \n",
        "Weight_array = W.eval(session=sess)\n",
        "bias_array = b.eval(session=sess)\n",
        "sess.close()     "
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at step:  0  is  305.429\n",
            "W values : [[0.16808607]\n",
            " [0.16866677]\n",
            " [0.16635722]\n",
            " [0.17008267]]\n",
            "b values : [0.33661488]\n",
            "Training loss at step:  5  is  282.72018\n",
            "W values : [[0.7502235 ]\n",
            " [0.7528154 ]\n",
            " [0.74250716]\n",
            " [0.7591351 ]]\n",
            "b values : [1.5024234]\n",
            "Training loss at step:  10  is  276.39557\n",
            "W values : [[1.0574409]\n",
            " [1.0610943]\n",
            " [1.0465648]\n",
            " [1.070002 ]]\n",
            "b values : [2.1176674]\n",
            "Training loss at step:  15  is  274.6341\n",
            "W values : [[1.2195718]\n",
            " [1.2237855]\n",
            " [1.2070284]\n",
            " [1.234059 ]]\n",
            "b values : [2.4423563]\n",
            "Training loss at step:  20  is  274.14352\n",
            "W values : [[1.305135 ]\n",
            " [1.3096443]\n",
            " [1.2917114]\n",
            " [1.3206384]]\n",
            "b values : [2.613708]\n",
            "Training loss at step:  25  is  274.0069\n",
            "W values : [[1.3502902]\n",
            " [1.3549556]\n",
            " [1.3364019]\n",
            " [1.36633  ]]\n",
            "b values : [2.7041373]\n",
            "Training loss at step:  30  is  273.9688\n",
            "W values : [[1.3741204]\n",
            " [1.3788681]\n",
            " [1.3599869]\n",
            " [1.3904433]]\n",
            "b values : [2.7518601]\n",
            "Training loss at step:  35  is  273.95825\n",
            "W values : [[1.3866965]\n",
            " [1.3914877]\n",
            " [1.3724337]\n",
            " [1.403169 ]]\n",
            "b values : [2.7770457]\n",
            "Training loss at step:  40  is  273.95526\n",
            "W values : [[1.3933333]\n",
            " [1.3981476]\n",
            " [1.3790022]\n",
            " [1.4098849]]\n",
            "b values : [2.790337]\n",
            "Training loss at step:  45  is  273.95444\n",
            "W values : [[1.3968359]\n",
            " [1.4016625]\n",
            " [1.3824687]\n",
            " [1.4134291]]\n",
            "b values : [2.7973511]\n",
            "Training loss at step:  50  is  273.95422\n",
            "W values : [[1.3986844]\n",
            " [1.4035174]\n",
            " [1.384298 ]\n",
            " [1.4152998]]\n",
            "b values : [2.801053]\n",
            "Training loss at step:  55  is  273.95416\n",
            "W values : [[1.3996599]\n",
            " [1.4044963]\n",
            " [1.3852633]\n",
            " [1.416287 ]]\n",
            "b values : [2.8030066]\n",
            "Training loss at step:  60  is  273.95416\n",
            "W values : [[1.4001747]\n",
            " [1.4050128]\n",
            " [1.3857728]\n",
            " [1.4168079]]\n",
            "b values : [2.8040378]\n",
            "Training loss at step:  65  is  273.95413\n",
            "W values : [[1.4004464]\n",
            " [1.4052855]\n",
            " [1.3860416]\n",
            " [1.4170829]]\n",
            "b values : [2.8045819]\n",
            "Training loss at step:  70  is  273.95416\n",
            "W values : [[1.4005898]\n",
            " [1.4054295]\n",
            " [1.3861834]\n",
            " [1.4172281]]\n",
            "b values : [2.804869]\n",
            "Training loss at step:  75  is  273.95413\n",
            "W values : [[1.4006655]\n",
            " [1.4055053]\n",
            " [1.3862581]\n",
            " [1.4173048]]\n",
            "b values : [2.8050203]\n",
            "Training loss at step:  80  is  273.95416\n",
            "W values : [[1.4007055]\n",
            " [1.4055452]\n",
            " [1.3862975]\n",
            " [1.4173453]]\n",
            "b values : [2.8051002]\n",
            "Training loss at step:  85  is  273.95413\n",
            "W values : [[1.4007267]\n",
            " [1.4055665]\n",
            " [1.3863182]\n",
            " [1.4173666]]\n",
            "b values : [2.8051426]\n",
            "Training loss at step:  90  is  273.95413\n",
            "W values : [[1.4007378]\n",
            " [1.4055775]\n",
            " [1.386329 ]\n",
            " [1.4173781]]\n",
            "b values : [2.8051648]\n",
            "Training loss at step:  95  is  273.95413\n",
            "W values : [[1.4007435]\n",
            " [1.4055836]\n",
            " [1.3863347]\n",
            " [1.4173841]]\n",
            "b values : [2.8051767]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr-XR1k_MuMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DOL2ncA1OB7q"
      },
      "source": [
        "### Get the shapes and values of W and b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZGvtyTeuOB7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "83512c45-ce4e-4efd-f33c-ed6dcf78ba6c"
      },
      "source": [
        "print('shape of W : ', Weight_array.shape )\n",
        "print('shape of b : ', bias_array.shape )\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of W :  (4, 1)\n",
            "shape of b :  (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUnoWRywZrjf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhDtOv5UOB7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5024a2a7-2d88-4402-d57a-cdda770edcf8"
      },
      "source": [
        "Weight_array"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4007461],\n",
              "       [1.4055864],\n",
              "       [1.3863372],\n",
              "       [1.4173869]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6S9NiuKf8Jq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "333077e0-ff59-4b94-a633-c0b76782747a"
      },
      "source": [
        "bias_array"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.805182], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZscomZ8YoVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ERq9GOKKciho"
      },
      "source": [
        "### Model Prediction on 1st Examples in Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKGvUWahcihp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "688cb0a8-15af-4786-faa8-dab7bbf56f9e"
      },
      "source": [
        "ypred = tf.add(tf.matmul(X_test,Weight_array),bias_array,name='output')\n",
        "\n",
        "sess1 = tf.Session()\n",
        "ypred.eval(session=sess1)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.61012  ],\n",
              "       [5.61028  ],\n",
              "       [5.6102724],\n",
              "       [5.6102104],\n",
              "       [5.610093 ],\n",
              "       [5.6102786],\n",
              "       [5.61026  ],\n",
              "       [5.6102896],\n",
              "       [5.610265 ],\n",
              "       [5.6102824],\n",
              "       [5.6102867],\n",
              "       [5.6102858],\n",
              "       [5.6102962],\n",
              "       [5.610273 ],\n",
              "       [5.610194 ],\n",
              "       [5.6102886],\n",
              "       [5.610196 ],\n",
              "       [5.6102557],\n",
              "       [5.609956 ],\n",
              "       [5.610261 ],\n",
              "       [5.610283 ],\n",
              "       [5.610278 ],\n",
              "       [5.6101937],\n",
              "       [5.61026  ],\n",
              "       [5.6102185],\n",
              "       [5.61022  ],\n",
              "       [5.6099806],\n",
              "       [5.610267 ],\n",
              "       [5.609865 ],\n",
              "       [5.609933 ],\n",
              "       [5.610265 ],\n",
              "       [5.610116 ],\n",
              "       [5.610282 ],\n",
              "       [5.6102037],\n",
              "       [5.6102133],\n",
              "       [5.610287 ],\n",
              "       [5.61028  ],\n",
              "       [5.6102667],\n",
              "       [5.6102686],\n",
              "       [5.6102657],\n",
              "       [5.610289 ],\n",
              "       [5.61026  ],\n",
              "       [5.610273 ],\n",
              "       [5.610262 ],\n",
              "       [5.6102376],\n",
              "       [5.610199 ],\n",
              "       [5.610245 ],\n",
              "       [5.6102514],\n",
              "       [5.610198 ],\n",
              "       [5.6102805],\n",
              "       [5.609797 ],\n",
              "       [5.610262 ],\n",
              "       [5.610284 ],\n",
              "       [5.6102953],\n",
              "       [5.610285 ],\n",
              "       [5.6102858],\n",
              "       [5.6102095],\n",
              "       [5.610249 ],\n",
              "       [5.6101   ],\n",
              "       [5.6102695],\n",
              "       [5.610284 ],\n",
              "       [5.610053 ],\n",
              "       [5.610266 ],\n",
              "       [5.6102605],\n",
              "       [5.610284 ],\n",
              "       [5.610269 ],\n",
              "       [5.6102724],\n",
              "       [5.610253 ],\n",
              "       [5.6102796],\n",
              "       [5.6102886],\n",
              "       [5.610285 ],\n",
              "       [5.6102467],\n",
              "       [5.6102824],\n",
              "       [5.610135 ],\n",
              "       [5.6102777],\n",
              "       [5.610286 ],\n",
              "       [5.610279 ],\n",
              "       [5.6102643],\n",
              "       [5.6102853],\n",
              "       [5.6102915],\n",
              "       [5.6102734],\n",
              "       [5.610241 ],\n",
              "       [5.609555 ],\n",
              "       [5.610247 ],\n",
              "       [5.6102033],\n",
              "       [5.6102567],\n",
              "       [5.6102633],\n",
              "       [5.610263 ],\n",
              "       [5.610259 ],\n",
              "       [5.6102505],\n",
              "       [5.610263 ],\n",
              "       [5.61018  ],\n",
              "       [5.610153 ],\n",
              "       [5.61029  ],\n",
              "       [5.610201 ],\n",
              "       [5.6102786],\n",
              "       [5.610139 ],\n",
              "       [5.610275 ],\n",
              "       [5.6102777],\n",
              "       [5.6096306],\n",
              "       [5.610282 ],\n",
              "       [5.6102676],\n",
              "       [5.6102676],\n",
              "       [5.610193 ],\n",
              "       [5.610243 ],\n",
              "       [5.610273 ],\n",
              "       [5.61027  ],\n",
              "       [5.61028  ],\n",
              "       [5.6102676],\n",
              "       [5.6102805],\n",
              "       [5.6102695],\n",
              "       [5.6102676],\n",
              "       [5.6101623],\n",
              "       [5.6102448],\n",
              "       [5.610257 ],\n",
              "       [5.610263 ],\n",
              "       [5.6102724],\n",
              "       [5.6100545],\n",
              "       [5.6102834],\n",
              "       [5.610133 ],\n",
              "       [5.610281 ],\n",
              "       [5.61028  ],\n",
              "       [5.6102476],\n",
              "       [5.610206 ],\n",
              "       [5.6102834],\n",
              "       [5.6101804],\n",
              "       [5.6101756],\n",
              "       [5.6099157],\n",
              "       [5.610136 ],\n",
              "       [5.610279 ],\n",
              "       [5.610179 ],\n",
              "       [5.61027  ],\n",
              "       [5.6102905],\n",
              "       [5.610278 ],\n",
              "       [5.6102867],\n",
              "       [5.610222 ],\n",
              "       [5.6102448],\n",
              "       [5.610252 ],\n",
              "       [5.6102176],\n",
              "       [5.609844 ],\n",
              "       [5.6102543],\n",
              "       [5.610257 ],\n",
              "       [5.610281 ],\n",
              "       [5.610279 ],\n",
              "       [5.6102533],\n",
              "       [5.6102405],\n",
              "       [5.610278 ],\n",
              "       [5.610291 ],\n",
              "       [5.6102867],\n",
              "       [5.6102657],\n",
              "       [5.6101837],\n",
              "       [5.6102676],\n",
              "       [5.610244 ],\n",
              "       [5.610277 ],\n",
              "       [5.610259 ],\n",
              "       [5.6102896],\n",
              "       [5.6102705],\n",
              "       [5.610256 ],\n",
              "       [5.610238 ],\n",
              "       [5.6102524],\n",
              "       [5.610262 ],\n",
              "       [5.610259 ],\n",
              "       [5.6102867],\n",
              "       [5.6102667],\n",
              "       [5.6102815],\n",
              "       [5.610282 ],\n",
              "       [5.610261 ],\n",
              "       [5.6102934],\n",
              "       [5.6102858],\n",
              "       [5.6102953],\n",
              "       [5.6102524],\n",
              "       [5.6102886],\n",
              "       [5.6102853],\n",
              "       [5.6102924],\n",
              "       [5.6102877],\n",
              "       [5.6102867],\n",
              "       [5.610283 ],\n",
              "       [5.6102924],\n",
              "       [5.610207 ],\n",
              "       [5.6102552],\n",
              "       [5.6102734],\n",
              "       [5.61028  ],\n",
              "       [5.610269 ],\n",
              "       [5.610288 ],\n",
              "       [5.610265 ],\n",
              "       [5.6102943],\n",
              "       [5.610202 ],\n",
              "       [5.610287 ],\n",
              "       [5.6102343],\n",
              "       [5.610259 ],\n",
              "       [5.6102896],\n",
              "       [5.6102796],\n",
              "       [5.6102877],\n",
              "       [5.610263 ],\n",
              "       [5.6102743],\n",
              "       [5.6102858],\n",
              "       [5.6102934],\n",
              "       [5.610159 ],\n",
              "       [5.6102657],\n",
              "       [5.610276 ],\n",
              "       [5.6102962],\n",
              "       [5.610283 ],\n",
              "       [5.6102905],\n",
              "       [5.6102185],\n",
              "       [5.610287 ],\n",
              "       [5.6101594],\n",
              "       [5.61022  ],\n",
              "       [5.610276 ],\n",
              "       [5.610278 ],\n",
              "       [5.6102533],\n",
              "       [5.6102924],\n",
              "       [5.6102786],\n",
              "       [5.61022  ],\n",
              "       [5.610259 ],\n",
              "       [5.6102552],\n",
              "       [5.610263 ],\n",
              "       [5.6101975],\n",
              "       [5.610278 ],\n",
              "       [5.610235 ],\n",
              "       [5.6102724],\n",
              "       [5.6101913],\n",
              "       [5.6102715],\n",
              "       [5.6100326],\n",
              "       [5.610276 ],\n",
              "       [5.6102934],\n",
              "       [5.6102867],\n",
              "       [5.610247 ],\n",
              "       [5.6102533],\n",
              "       [5.610267 ],\n",
              "       [5.610244 ],\n",
              "       [5.6102505],\n",
              "       [5.610281 ],\n",
              "       [5.610278 ],\n",
              "       [5.610259 ],\n",
              "       [5.6102223],\n",
              "       [5.610217 ],\n",
              "       [5.610292 ],\n",
              "       [5.6102624],\n",
              "       [5.61026  ],\n",
              "       [5.6102924],\n",
              "       [5.6102605],\n",
              "       [5.6102147],\n",
              "       [5.6102943],\n",
              "       [5.610277 ],\n",
              "       [5.610207 ],\n",
              "       [5.610265 ],\n",
              "       [5.6102715],\n",
              "       [5.610278 ],\n",
              "       [5.6102924],\n",
              "       [5.610283 ],\n",
              "       [5.61028  ],\n",
              "       [5.608617 ],\n",
              "       [5.6102886],\n",
              "       [5.6101246],\n",
              "       [5.6102786],\n",
              "       [5.610222 ],\n",
              "       [5.610159 ],\n",
              "       [5.610125 ],\n",
              "       [5.6102614],\n",
              "       [5.6102734],\n",
              "       [5.609973 ],\n",
              "       [5.610281 ],\n",
              "       [5.6102104],\n",
              "       [5.6101704],\n",
              "       [5.610221 ],\n",
              "       [5.6102595],\n",
              "       [5.6102543],\n",
              "       [5.610154 ],\n",
              "       [5.610285 ],\n",
              "       [5.6102667],\n",
              "       [5.610284 ],\n",
              "       [5.6102896],\n",
              "       [5.6102905],\n",
              "       [5.6102552],\n",
              "       [5.6102805],\n",
              "       [5.610276 ],\n",
              "       [5.6102533],\n",
              "       [5.6102734],\n",
              "       [5.6102443],\n",
              "       [5.6102858],\n",
              "       [5.610281 ],\n",
              "       [5.6102357],\n",
              "       [5.610257 ],\n",
              "       [5.6102514],\n",
              "       [5.610263 ],\n",
              "       [5.610262 ],\n",
              "       [5.610276 ],\n",
              "       [5.610265 ],\n",
              "       [5.6101522],\n",
              "       [5.6102734],\n",
              "       [5.6101103],\n",
              "       [5.6102877],\n",
              "       [5.606498 ],\n",
              "       [5.6102557],\n",
              "       [5.6102276],\n",
              "       [5.6099596],\n",
              "       [5.6102734],\n",
              "       [5.610257 ],\n",
              "       [5.610271 ],\n",
              "       [5.6102657]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJRBuqXhOB7_"
      },
      "source": [
        "## Classification using tf.Keras\n",
        "\n",
        "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O0g6lorycihf"
      },
      "source": [
        "### Load the given Iris data using pandas (Iris.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6xFvb5sRcihg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7347c260-f3b3-4a66-aadd-8cd801c18d79"
      },
      "source": [
        "data_I.head()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
              "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SAB--Qdwcihm"
      },
      "source": [
        "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IJr5dYnocihm",
        "colab": {}
      },
      "source": [
        "X = data_I.iloc[:,1:5].values\n",
        "y = data_I.iloc[:,5].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder =  LabelEncoder()\n",
        "y1 = encoder.fit_transform(y)\n",
        "\n",
        "Y = pd.get_dummies(y1).values\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D95nY5ILcihj"
      },
      "source": [
        "### Splitting the data into feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RyMQoLMucihj",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b22qpC5xcihr"
      },
      "source": [
        "###  Building Model in tf.keras\n",
        "\n",
        "Build a Linear Classifier model  <br>\n",
        "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
        "2. Apply Softmax on Dense Layer outputs <br>\n",
        "3. Use SGD as Optimizer\n",
        "4. Use categorical_crossentropy as loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6_RsU7Qmbr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hov_UFnUciht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "1e0bf5c1-cc88-43ab-a1bd-9443aa5f2704"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=4, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 12:55:26.873008 139870843578240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 12:55:26.876922 139870843578240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 12:55:26.916698 139870843578240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 12:55:26.943414 139870843578240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5FdzqIKcihw"
      },
      "source": [
        "### Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4qLEdHPscihx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-SgSSdRcih5"
      },
      "source": [
        "### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBgKZkhkcih6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "02d6ac3b-d127-4aa3-d16a-354e04874354"
      },
      "source": [
        "ypred = model.predict(X_test)\n",
        "ypred"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46154684, 0.5246377 , 0.01381546],\n",
              "       [0.39297545, 0.5835152 , 0.02350934],\n",
              "       [0.40914512, 0.53168684, 0.05916805],\n",
              "       [0.43285817, 0.56058645, 0.00655541],\n",
              "       [0.40087697, 0.5311961 , 0.06792691],\n",
              "       [0.48848125, 0.5034019 , 0.00811683],\n",
              "       [0.39637178, 0.5312695 , 0.07235871],\n",
              "       [0.41929275, 0.5667911 , 0.01391613],\n",
              "       [0.40539637, 0.58092904, 0.01367459],\n",
              "       [0.41304618, 0.5654518 , 0.02150198],\n",
              "       [0.4602877 , 0.5284081 , 0.0113042 ],\n",
              "       [0.43185514, 0.55244356, 0.01570128],\n",
              "       [0.43840697, 0.5454455 , 0.01614751],\n",
              "       [0.41306707, 0.57134545, 0.01558746],\n",
              "       [0.44169563, 0.54246664, 0.01583777],\n",
              "       [0.39268565, 0.53554195, 0.07177238],\n",
              "       [0.4391518 , 0.54340434, 0.01744387],\n",
              "       [0.44938207, 0.52990097, 0.02071691],\n",
              "       [0.38764217, 0.5360903 , 0.07626753],\n",
              "       [0.3956174 , 0.5399035 , 0.06447913],\n",
              "       [0.46687117, 0.51741946, 0.01570931],\n",
              "       [0.46391255, 0.5177212 , 0.0183663 ],\n",
              "       [0.4266577 , 0.51349664, 0.05984564],\n",
              "       [0.4008869 , 0.5166482 , 0.08246493],\n",
              "       [0.43365017, 0.5514018 , 0.01494807],\n",
              "       [0.40536526, 0.5088536 , 0.08578109],\n",
              "       [0.42959642, 0.5164455 , 0.05395804],\n",
              "       [0.4229933 , 0.5585584 , 0.01844837],\n",
              "       [0.42110953, 0.5419253 , 0.03696517],\n",
              "       [0.4041862 , 0.53089976, 0.064914  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P32ASP1Vjt0a"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8rd0jjAjyTR",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump(model, open(\"model.pkl\",\"wb\" , ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XiipRpe7rbVh"
      },
      "source": [
        "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
        "\n",
        "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v5Du3lubr4sA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}